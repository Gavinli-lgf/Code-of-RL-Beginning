policy iteration的代码跟value iteration很像
我们可以这样来理解：value iteration是为了policy iteration 服务的
policy iteration就是把一整个迭代过程，更进一步细化
先进行策略评估，
再根据这个评估，去选择更好的策略，循环往复

这个方法会更加的容易让人接受
无论从数学上还是从代码上
policy iteration都会更清晰一点

我们来看看赵老师的伪代码，同样的，我们现在有的是整个已知的model
我们要求的是贝尔曼最优方程的解，以及一个最优的策略。

当v值没有收敛时，执行外层大循环
随机生成一个策略，进行策略评估 policy iteration
所用的方法是迭代式的贝尔曼方程求解

之后进行策略改进。
注意看到，这里其实并不是“迭代”式，而是定义式。因为所有的要求的东西，在上式已经知道了。
这里是已知所有的state value，求action value

		还有一点要注意，这里的红框是从属于外部循环的
		意味着对于每个state，进行当前迭代后
		立马进行策略更新，此时的策略是基于“当前瞬间”的新知识，和过往的“旧”经验，所产生的。

		如果把红框写在外面，从属于最外部的循环
		意味着，进行了完整的一轮贝尔曼迭代
		使用新知识产生新策略，
		再应用于下一轮的贝尔曼迭代中
		
		和value iteration 不同的是，无论红框在哪，这个新改进的policy都不会参与到下一次的action value的生成当中。

	红框写在外面和里面最大区别是
	最外层迭代次数的不一样
	并不影响收敛性。

	下面我们来看代码：