下面我们进入到4.1 value iteration 值迭代算法
讲解伪代码之前，我们要明晰一点。
我们未来要写的所有所有所有代码，
目的都是对“贝尔曼最优方程”进行求解

来看看老师的伪代码：
我们在写一个算法之前，一定要明晰：
我们现在有什么
我们要做什么
我们现在有的是Model，即知道每个state action对应的瞬时得分
我们直接求解贝尔曼最优方程
即最优的state value和policy

再看到循环体部分
当它不收敛的时候，执行循环：
   对于每一个state 
       在每个state中，枚举所有动作
            对于每一个（state，action）pair
            进行贝尔曼方程的迭代

            这里要注意观察，这里迭代的是action value，在4.2里面会变成state value，要注意对比学习。

		还有一点要注意，这里的红框是从属于外部循环的
		意味着对于每个state，进行当前迭代后
		立马进行策略更新，并且将该策略应用于下一个state的迭代。
		此时的策略是基于“当前瞬间”的新知识和过往的“旧”经验组合，所产生的。

		如果把红框写在外面，从属于最外部的循环
		意味着，进行了完整的一轮贝尔曼迭代
		使用新知识产生新策略，
		再应用于下一轮的贝尔曼迭代中

        在model base的情况下，每进行一次迭代，目标state value就越近
		二者最终都能收敛，所影响的只是迭代的轮次不同。

我们看到代码部分

